{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import time\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import json\n",
    "import requests\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2597806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "date_start = datetime.datetime(2020, 7, 1, 0)\n",
    "date_end = datetime.datetime(2020, 7, 8, 0)\n",
    "date_start_str = date_start.strftime('%Y-%m-%d-%H')\n",
    "date_end_str = date_end.strftime('%Y-%m-%d-%H')\n",
    "wiki_db = 'wikidata'\n",
    "\n",
    "size=50\n",
    "min_count=5\n",
    "\n",
    "PATH_file = '../output/models/'\n",
    "# # ## one week\n",
    "filename = os.path.join(\n",
    "    PATH_file,\n",
    "    'embedding-w2v_%s_%s_%s_args-%s-%s.bin'%(\n",
    "        wiki_db,date_start_str,date_end_str,\n",
    "        size,min_count\n",
    "    )\n",
    ")\n",
    "model = fasttext.load_model(filename)\n",
    "print(len(model.get_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the list of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test: pandemic, disease\n",
    "list_qid_seed = ['Q81068910','Q84263196']\n",
    "N_nn = 200\n",
    "list_wikis = ['enwiki','dewiki','frwiki','eswiki','ruwiki','zhwiki','ptwiki','arwiki','bnwiki','hiwiki']\n",
    "list_keywords = ['covid','corona','cov'] ## if labels contain these words, remove item\n",
    "N_max = 10\n",
    "formatted = False\n",
    "\n",
    "# ## set1: pandemic, disease\n",
    "# list_qid_seed = ['Q81068910','Q84263196']\n",
    "# N_nn = 1000\n",
    "# list_wikis = ['enwiki','dewiki','frwiki','eswiki','ruwiki','zhwiki','ptwiki','arwiki','bnwiki','hiwiki']\n",
    "# list_keywords = ['covid','corona','cov'] ## if labels contain these words, remove item\n",
    "# N_max = 200\n",
    "# formatted = False\n",
    "\n",
    "# # set1: xenophobia\n",
    "# list_qid_seed = ['Q84318312']\n",
    "# N_nn = 200 ## initial number of nearet neighbors\n",
    "# list_wikis = ['enwiki']\n",
    "# list_keywords = []## if labels contain these words, remove item\n",
    "# N_max=100 ## how many pages to keep from candidates\n",
    "# formatted = False ## if formatting for wikitext\n",
    "\n",
    "# # set: gendered impact of the covid pandemic\n",
    "# list_qid_seed = ['Q89666473']\n",
    "# N_nn = 200 ## initial number of nearet neighbors\n",
    "# list_wikis = ['enwiki']\n",
    "# list_keywords = []## if labels contain these words, remove item\n",
    "# N_max=100 ## how many pages to keep from candidates\n",
    "# formatted = False ## if formatting for wikitext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query related items and get additional information on articles in respective wikis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get nearest neighbors of seed-article and query titles/sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getItems(list_qids,list_wikis=None, n_batch=30):\n",
    "    '''\n",
    "    for list of qids, get:\n",
    "    - label\n",
    "    - description\n",
    "    - titles (sitelinks)\n",
    "    if list_wikis is specified, only get titles for specific wikis (e.g. enwiki)\n",
    "    \n",
    "    process the qids in batches of n_batches (at most 50, but make smaller to be sure).\n",
    "    use: https://www.mediawiki.org/wiki/Wikibase/API\n",
    "    '''\n",
    "    api_url_base = 'https://wikidata.org/w/api.php'\n",
    "    ## split into batches\n",
    "    list_qids_split = np.array_split(list_qids,math.ceil(len(list_qids)/n_batch))\n",
    "    list_items = []\n",
    "    i_qid=0    \n",
    "    for list_qids_batch in list_qids_split:\n",
    "        \n",
    "        params = {\n",
    "            'action':'wbgetentities',\n",
    "            'props':'sitelinks|labels|descriptions',\n",
    "            'languages':'en',\n",
    "            'format' : 'json',\n",
    "            'ids':'|'.join(list_qids_batch),\n",
    "        }\n",
    "        if list_wikis != None:\n",
    "            params['sitefilter']:'|'.join(list_wikis)\n",
    "\n",
    "        response = requests.get( api_url_base,params=params)\n",
    "        result=json.loads(response.text)\n",
    "        \n",
    "        for qid in list_qids_batch:\n",
    "            dict_item = {'qid':qid}\n",
    "\n",
    "            ## titles via sitelinks\n",
    "            dict_title = {} ## collect all titles of an item via sitelinks\n",
    "            titles = result['entities'].get(qid,{}).get('sitelinks',{})\n",
    "            for wiki, wiki_title in titles.items():\n",
    "                title = wiki_title['title'].replace(' ','_')\n",
    "                dict_title[wiki] = title\n",
    "            dict_item['titles'] = dict_title\n",
    "            \n",
    "            ## labels: put '-' if not available (so far only en)\n",
    "            label = result['entities'].get(qid,{}).get('labels',{}).get('en',{}).get('value','-')\n",
    "            dict_item['label'] = label\n",
    "            \n",
    "            ## description: put '-' if not available (so far only en)\n",
    "            description = result['entities'].get(qid,{}).get('descriptions',{}).get('en',{}).get('value','-')\n",
    "            dict_item['description'] = description\n",
    "            \n",
    "            list_items += [dict_item]\n",
    "            \n",
    "            i_qid+=1    \n",
    "    return list_items\n",
    "            \n",
    "\n",
    "def items_add_titles(list_items, list_wikis = ['enwiki']):\n",
    "    '''\n",
    "    \n",
    "    add information about a list of qids:\n",
    "    - label\n",
    "    - description\n",
    "    - article-titles (sitelinks) for a set of wikis\n",
    "    '''\n",
    "    \n",
    "    list_qids = [h['qid'] for h in list_items]\n",
    "    list_items_meta = getItems(list_qids, list_wikis=list_wikis)\n",
    "    \n",
    "    list_items_new = list_items\n",
    "    for i_item, item in enumerate(list_items):\n",
    "        dict_titles = {}\n",
    "        for wiki in list_wikis:\n",
    "            title =  list_items_meta[i_item]['titles'].get(wiki,'-')\n",
    "            dict_titles[wiki] = title\n",
    "        list_items_new[i_item]['titles'] = dict_titles\n",
    "        list_items_new[i_item]['label'] = list_items_meta[i_item]['label']\n",
    "        list_items_new[i_item]['description'] = list_items_meta[i_item]['description']\n",
    "        \n",
    "        ## add text\n",
    "        text = list_items_new[i_item]['label']\n",
    "        if text == '-':\n",
    "            text_tmp =  list_items_new[i_item]['description']\n",
    "            if text_tmp!='-':\n",
    "                text = text_tmp          \n",
    "\n",
    "        for wiki in list_wikis:\n",
    "            if text != '-':\n",
    "                break\n",
    "            else:\n",
    "                text_tmp = list_items_new[i_item]['titles'][wiki]\n",
    "                if text_tmp!='-':\n",
    "                    text = text_tmp+' [title-%s]'%(wiki)\n",
    "        list_items_new[i_item]['text'] = text\n",
    "    return list_items_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q84263196', 'score': 0.9195318818092346, 'titles': {'enwiki': 'Coronavirus_disease_2019', 'dewiki': 'COVID-19'}, 'label': 'COVID-19', 'description': 'zoonotic respiratory syndrome and infectious disease in humans, caused by SARS coronavirus 2', 'text': 'COVID-19'}, {'qid': 'Q81068910', 'score': 0.9195318818092346, 'titles': {'enwiki': 'COVID-19_pandemic', 'dewiki': 'COVID-19-Pandemie'}, 'label': 'COVID-19 pandemic', 'description': 'ongoing pandemic of COVID-19', 'text': 'COVID-19 pandemic'}]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dict_nn_score = defaultdict(list)\n",
    "for qid in list_qid_seed:\n",
    "    qid_nn = model.get_nearest_neighbors(qid,k=N_nn)\n",
    "    for score_tmp,qid_tmp in qid_nn:\n",
    "        dict_nn_score[qid_tmp] += [score_tmp] \n",
    "        \n",
    "list_nn_qid_tmp = []\n",
    "list_nn_score_tmp = []\n",
    "for qid_tmp,score_tmp in dict_nn_score.items():\n",
    "    list_nn_qid_tmp+=[qid_tmp]\n",
    "    list_nn_score_tmp += [max(score_tmp)]\n",
    "    \n",
    "indsort = np.argsort(list_nn_score_tmp)[::-1][:N_nn]\n",
    "list_nn_qid = [list_nn_qid_tmp[i] for i in indsort]\n",
    "list_nn_score = [list_nn_score_tmp[i] for i in indsort]\n",
    "list_items = [\n",
    "    {'qid': list_nn_qid[i],\n",
    "     'score': list_nn_score[i]\n",
    "    } \n",
    "    for i in range(N_nn) \n",
    "]\n",
    "list_items = items_add_titles(list_items,list_wikis = list_wikis)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter list: disambiguation, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_filter_notext(\n",
    "    list_items):\n",
    "    '''filter all items that have\n",
    "    - no English label\n",
    "    - no description\n",
    "    - no existing title in any of the selected wikis\n",
    "    '''\n",
    "    list_items_new = []\n",
    "    for item in list_items:\n",
    "        if item['text'] != '-':\n",
    "            list_items_new += [item]\n",
    "    return list_items_new\n",
    "\n",
    "def items_filter_disambiguation(\n",
    "    list_items,\n",
    "    list_keywords=['disambiguation']):\n",
    "    s = re.compile('|'.join(list_keywords))\n",
    "    list_items_new = []\n",
    "    for item in list_items:\n",
    "        label = item['label']+' '+item['description']\n",
    "        if s.search(label.lower()) is None:\n",
    "            list_items_new += [item]\n",
    "    return list_items_new\n",
    "\n",
    "def items_filter_keywords(\n",
    "    list_items,\n",
    "    list_keywords=[] ):\n",
    "    if len(list_keywords) == 0:\n",
    "        return list_items\n",
    "    else:\n",
    "        s = re.compile('|'.join(list_keywords))\n",
    "        list_items_new = []\n",
    "        for item in list_items:\n",
    "            label = item['label']\n",
    "            if s.search(label.lower()) is None:\n",
    "                list_items_new += [item]\n",
    "        return list_items_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q103177', 'score': 0.8773841857910156, 'titles': {'enwiki': 'Severe_acute_respiratory_syndrome', 'dewiki': 'Schweres_akutes_Atemwegssyndrom'}, 'label': 'severe acute respiratory syndrome', 'description': 'viral respiratory disease', 'text': 'severe acute respiratory syndrome'}, {'qid': 'Q83264280', 'score': 0.8562499284744263, 'titles': {'enwiki': 'Huanan_Seafood_Wholesale_Market', 'dewiki': 'Feinkost-Nassmarkt_in_Wuhan'}, 'label': 'Huanan Seafood Wholesale Market', 'description': 'market in Wuhan, Hubei, China', 'text': 'Huanan Seafood Wholesale Market'}]\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "## filter if the label contains a keywords as substring\n",
    "list_items_filtered = list(list_items)\n",
    "list_items_filtered = items_filter_notext(list_items_filtered)\n",
    "list_items_filtered = items_filter_disambiguation(list_items_filtered)\n",
    "list_items_filtered = items_filter_keywords(list_items_filtered,list_keywords = list_keywords)\n",
    "list_items = list(list_items_filtered)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRedirects(page,project):\n",
    "    '''\n",
    "    get all redirects (upto 500)\n",
    "    for a given page \n",
    "    '''\n",
    "    base_url = 'https://%s.org/w/api.php?action=query&titles=%s&prop=redirects&rdlimit=500&format=json' % (project,page)\n",
    "    data = [p['title'] for p in list(requests.get(base_url).json()['query']['pages'].values())[0]['redirects']]\n",
    "    return data\n",
    "    \n",
    "\n",
    "def getViews(page,start,end,project):\n",
    "    \"\"\"\n",
    "    get pageviews using this API https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews \n",
    "    page: str (article name)\n",
    "    start: str start date YYYYMMDD (20200101)\n",
    "    end: str end date YYYYMMDD (20200103)\n",
    "    project: str, ex: en.wikipedia (project does not include .org)\n",
    "\n",
    "    \"\"\"\n",
    "    base_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/%s/all-access/all-agents/%s/daily/%s/%s\" % (project,page,start,end)\n",
    "    try:\n",
    "        data = requests.get(base_url).json()['items']\n",
    "        df = pd.DataFrame(data) [['views','timestamp']]\n",
    "        df.rename(columns={'views':page},inplace=True)\n",
    "    except KeyError:\n",
    "        ## no pageviews- we have to set 1 date with 0 counts\n",
    "        df = pd.DataFrame(columns=[page,'timestamp'],index=[0])\n",
    "        df.iloc[0,0] = 0\n",
    "        df.iloc[0,1] = start\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H')\n",
    "    df.set_index('timestamp',inplace=True)\n",
    "    return df\n",
    "\n",
    "def getViewsMultiples(pages,start,end,project):\n",
    "    \"\"\"\n",
    "    Get page views for a list of pages \n",
    "    pages: list (list of article's titles) ex: ['Chile','Brasil','Argentina']\n",
    "    start: str start date YYYYMMDD (20200101)\n",
    "    end: str end date YYYYMMDD (20200103)\n",
    "    project: str, ex: en.wikipedia (project does not include .org)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for page in pages:\n",
    "        try:\n",
    "            results.append(getViews(page,start,end,project))\n",
    "        except:\n",
    "            pass\n",
    "    return pd.concat(results,axis=1)\n",
    "\n",
    "def getViewsWithRedirects(page,start,end,project):\n",
    "    \"\"\"\n",
    "    Get all redirects going to 'page' and get pageviews for that article\n",
    "    page: str (article's title)\n",
    "    start: str start date YYYYMMDD (20200101)\n",
    "    end: str end date YYYYMMDD (20200103)\n",
    "    project: str, ex: en.wikipedia (project does not include .org)\n",
    "    \"\"\"\n",
    "    #start list of page\n",
    "    pages = [page]\n",
    "    try:\n",
    "        #get all redirects to page \n",
    "        redirects = getRedirects(page=page,project=project)     \n",
    "    except:\n",
    "        #if getRedirects gives an error, we assume that there no pages redirecting to page\n",
    "        redirects = []\n",
    "    pages.extend(redirects)\n",
    "    ## get pages views for all articles\n",
    "    results = getViewsMultiples(pages=pages,start=start,end=end,project=project)\n",
    "    #sum all pages views\n",
    "    results = pd.DataFrame(results.sum(axis=1))\n",
    "    results.rename(columns={0:page},inplace=True)\n",
    "    return results      \n",
    "\n",
    "def items_add_pageviews(list_items,start,end,N_max = 10):\n",
    "    '''\n",
    "    for a list of records [{'titles':{'enwiki':'Germany','dewiki':'Deutschland'}}, {}]\n",
    "    add pageviews\n",
    "    '''\n",
    "    \n",
    "    for i_item,item in enumerate(list_items[:N_max]):\n",
    "        dict_pageviews = {}\n",
    "        for wiki in list_wikis:\n",
    "            project = '%s.wikipedia' % wiki.replace('wiki','')\n",
    "            page = item['titles'][wiki]\n",
    "            if page == '-':\n",
    "                views = np.nan\n",
    "            else:\n",
    "                ## get pageviews per day\n",
    "                views_df = getViewsWithRedirects(page,start,end,project)\n",
    "                views = int(views_df.sum())\n",
    "            ## sum pageviews\n",
    "            dict_pageviews[wiki] = views\n",
    "        list_items[i_item]['pageviews'] = dict_pageviews\n",
    "    return list_items[:N_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q103177', 'score': 0.8773841857910156, 'titles': {'enwiki': 'Severe_acute_respiratory_syndrome', 'dewiki': 'Schweres_akutes_Atemwegssyndrom'}, 'label': 'severe acute respiratory syndrome', 'description': 'viral respiratory disease', 'text': 'severe acute respiratory syndrome', 'pageviews': {'enwiki': 29966, 'dewiki': 2965}}, {'qid': 'Q83264280', 'score': 0.8562499284744263, 'titles': {'enwiki': 'Huanan_Seafood_Wholesale_Market', 'dewiki': 'Feinkost-Nassmarkt_in_Wuhan'}, 'label': 'Huanan Seafood Wholesale Market', 'description': 'market in Wuhan, Hubei, China', 'text': 'Huanan Seafood Wholesale Market', 'pageviews': {'enwiki': 6752, 'dewiki': 122}}]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "start = str(date_start).replace('-','').split(' ')[0]\n",
    "end =str(date_end).replace('-','').split(' ')[0]\n",
    "list_items = items_add_pageviews(list_items,start,end,N_max=N_max)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200701'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add topics from wikidata topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(qid,p_min=0.1):\n",
    "#     api_url_base = 'https://tools.wmflabs.org/wiki-topic/api/v1/wikidata/topic'\n",
    "    api_url_base = 'https://wikidata-topic.wmcloud.org/api/v1/topic'\n",
    "    params = {\n",
    "        'qid':qid,\n",
    "         'threshold':0.1\n",
    "    }\n",
    "    ## query api: https://tools.wmflabs.org/wiki-topic/\n",
    "    response = requests.get( api_url_base,params=params)\n",
    "    result=json.loads(response.text)   \n",
    "    ## get scores (topic-probabilities) and topics\n",
    "    scores = [h['score'] for h in result['results']]\n",
    "    topics = [h['topic'] for h in result['results']]\n",
    "    ## get topic w maximum probability (if p exceeds p_min)\n",
    "    ind_max = np.argmax(scores)\n",
    "    score_max = scores[ind_max]\n",
    "    if score_max >= p_min:\n",
    "        topic = topics[ind_max]\n",
    "    else:\n",
    "        topic = '-'\n",
    "    return topic\n",
    "    \n",
    "def items_addTopics(list_items):\n",
    "    for i_item,item in enumerate(list_items):\n",
    "        qid = item['qid']\n",
    "        topic = getTopic(qid)\n",
    "        list_items[i_item]['topic'] = topic\n",
    "    return list_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'qid': 'Q103177', 'score': 0.8773841857910156, 'titles': {'enwiki': 'Severe_acute_respiratory_syndrome', 'dewiki': 'Schweres_akutes_Atemwegssyndrom'}, 'label': 'severe acute respiratory syndrome', 'description': 'viral respiratory disease', 'text': 'severe acute respiratory syndrome', 'pageviews': {'enwiki': 29966, 'dewiki': 2965}, 'topic': 'STEM.STEM*'}, {'qid': 'Q83264280', 'score': 0.8562499284744263, 'titles': {'enwiki': 'Huanan_Seafood_Wholesale_Market', 'dewiki': 'Feinkost-Nassmarkt_in_Wuhan'}, 'label': 'Huanan Seafood Wholesale Market', 'description': 'market in Wuhan, Hubei, China', 'text': 'Huanan Seafood Wholesale Market', 'pageviews': {'enwiki': 6752, 'dewiki': 122}, 'topic': 'History_and_Society.Society'}]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "list_items = items_addTopics(list_items)\n",
    "print(list_items[:2])\n",
    "print(len(list_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fromat and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikidata-item</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>topic</th>\n",
       "      <th>enwiki:title</th>\n",
       "      <th>enwiki:views</th>\n",
       "      <th>dewiki:title</th>\n",
       "      <th>dewiki:views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q103177</td>\n",
       "      <td>severe acute respiratory syndrome</td>\n",
       "      <td>0.877</td>\n",
       "      <td>STEM.STEM*</td>\n",
       "      <td>Severe acute respiratory syndrome</td>\n",
       "      <td>29966</td>\n",
       "      <td>Schweres akutes Atemwegssyndrom</td>\n",
       "      <td>2965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q83264280</td>\n",
       "      <td>Huanan Seafood Wholesale Market</td>\n",
       "      <td>0.856</td>\n",
       "      <td>History_and_Society.Society</td>\n",
       "      <td>Huanan Seafood Wholesale Market</td>\n",
       "      <td>6752</td>\n",
       "      <td>Feinkost-Nassmarkt in Wuhan</td>\n",
       "      <td>&lt;500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q834456</td>\n",
       "      <td>2002–2004 SARS outbreak</td>\n",
       "      <td>0.841</td>\n",
       "      <td>STEM.STEM*</td>\n",
       "      <td>2002–2004 SARS outbreak</td>\n",
       "      <td>28534</td>\n",
       "      <td>SARS-Pandemie 2002/2003</td>\n",
       "      <td>&lt;500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q101452</td>\n",
       "      <td>2009 swine flu pandemic</td>\n",
       "      <td>0.838</td>\n",
       "      <td>STEM.STEM*</td>\n",
       "      <td>2009 swine flu pandemic</td>\n",
       "      <td>74783</td>\n",
       "      <td>Pandemie H1N1 2009/10</td>\n",
       "      <td>&lt;500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q36855</td>\n",
       "      <td>swine influenza</td>\n",
       "      <td>0.833</td>\n",
       "      <td>STEM.STEM*</td>\n",
       "      <td>Swine influenza</td>\n",
       "      <td>28158</td>\n",
       "      <td>Schweineinfluenza</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Wikidata-item                              label  score  \\\n",
       "1       Q103177  severe acute respiratory syndrome  0.877   \n",
       "2     Q83264280    Huanan Seafood Wholesale Market  0.856   \n",
       "3       Q834456            2002–2004 SARS outbreak  0.841   \n",
       "4       Q101452            2009 swine flu pandemic  0.838   \n",
       "5        Q36855                    swine influenza  0.833   \n",
       "\n",
       "                         topic                       enwiki:title  \\\n",
       "1                   STEM.STEM*  Severe acute respiratory syndrome   \n",
       "2  History_and_Society.Society    Huanan Seafood Wholesale Market   \n",
       "3                   STEM.STEM*            2002–2004 SARS outbreak   \n",
       "4                   STEM.STEM*            2009 swine flu pandemic   \n",
       "5                   STEM.STEM*                    Swine influenza   \n",
       "\n",
       "  enwiki:views                     dewiki:title dewiki:views  \n",
       "1        29966  Schweres akutes Atemwegssyndrom         2965  \n",
       "2         6752      Feinkost-Nassmarkt in Wuhan         <500  \n",
       "3        28534          SARS-Pandemie 2002/2003         <500  \n",
       "4        74783            Pandemie H1N1 2009/10         <500  \n",
       "5        28158                Schweineinfluenza         1265  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "if formatted == True:\n",
    "    df['Wikidata-item'] = [ '[[wikidata:{0}|{0}]]'.format(h['qid']) for h in list_items ]\n",
    "else:\n",
    "    df['Wikidata-item'] = [ '{0}'.format(h['qid']) for h in list_items ]\n",
    "df['label'] = [ h['text'] for h in list_items ]\n",
    "df['score'] = [ '%.3f'%(h['score']) for h in list_items ]\n",
    "df['topic'] = [ h['topic'] for h in list_items ]\n",
    "for wiki in list_wikis:\n",
    "    if formatted == True:\n",
    "        df['%s:title'%(wiki)] = [  '[[%s:%s|%s]]'%(wiki[:-4],h['titles'][wiki],h['titles'][wiki].replace('_',' ')) if h['titles'][wiki]!='-' else '-'  for h in list_items ]\n",
    "    else:\n",
    "        df['%s:title'%(wiki)] = [  '%s'%(h['titles'][wiki].replace('_',' ')) if h['titles'][wiki]!='-' else '-'  for h in list_items ]\n",
    "\n",
    "    df['%s:views'%wiki] = [ '%s'%(h['pageviews'][wiki]) if (h['pageviews'][wiki])>=500 else '<500' if np.isnan((h['pageviews'][wiki]))==False else '-'   for h in list_items ]\n",
    "\n",
    "df.index = np.arange(1, len(df) + 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_save = os.path.join(\n",
    "    os.pardir,\n",
    "    'output',\n",
    "    'lists',\n",
    "    'reading-sessions-related-articles_%s_N%s_%s--%s_formatted-%s.csv'%(\n",
    "        '-'.join(list_qid_seed),\n",
    "        N_max,\n",
    "        str(date_start_str), str(date_end_str), formatted\n",
    "    )\n",
    ")\n",
    "df.to_csv(filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gensim",
   "language": "python",
   "name": "venv_gensim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
