{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading sessions from Pageview actor-table\n",
    "\n",
    "https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Traffic/Pageview_actor\n",
    "\n",
    "Generate reading sessions.\n",
    "- this produces a text-file on disk with a sequnence of qids separated by whitespace (each sequence is one line)\n",
    "- see the filtering parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import calendar\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import wmfdata.spark as wmfspark\n",
    "\n",
    "## defining the spark session\n",
    "spark_config = {}\n",
    "## regular\n",
    "# spark_config = {\n",
    "#     \"spark.driver.memory\": \"2g\",\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "#     \"spark.executor.memory\": \"8g\",\n",
    "#     \"spark.executor.cores\": 4,\n",
    "#     \"spark.sql.shuffle.partitions\": 256\n",
    "# }\n",
    "# ## big\n",
    "spark_config = {\n",
    "    \"spark.driver.memory\": \"4g\",\n",
    "    \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "    \"spark.executor.memory\": \"8g\",\n",
    "    \"spark.executor.cores\": 4,\n",
    "    \"spark.sql.shuffle.partitions\": 512\n",
    "}\n",
    "# spark_config = {\n",
    "#     \"spark.dynamicAllocation.maxExecutors\": 128,\n",
    "#     \"spark.executor.memory\": \"16g\",\n",
    "#     \"spark.driver.memory\": \"12g\",\n",
    "#     \"spark.executor.memoryOverhead\":\"4g\",\n",
    "#     \"spark.num.executors\":50,\n",
    "#     \"spark.driver.maxResultSize\":\"32g\",\n",
    "# }\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'notebook'\n",
    "# os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.5'\n",
    "\n",
    "spark = wmfspark.get_session(\n",
    "    app_name='Pyspark notebook', \n",
    "    extra_settings=spark_config\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining filter and maps\n",
    "def parse_requests(requests):\n",
    "    \"\"\"\n",
    "    do some initial parsing:\n",
    "    - drop pages without timestamp (we dont know which order)\n",
    "    \"\"\"\n",
    "    requests_clean = []\n",
    "    for r in requests:\n",
    "        if r['ts'] == None:\n",
    "            pass\n",
    "        else:\n",
    "            requests_clean += [r]\n",
    "    return requests_clean\n",
    "\n",
    "def filter_consecutive_articles(requests):\n",
    "    \"\"\"\n",
    "    Looking at the data, there are a lot of\n",
    "    sessions with the same article\n",
    "    requested 2 times in a row. This\n",
    "    does not make sense for training, so\n",
    "    lets collapse them into 1 request.\n",
    "    We compare qids\n",
    "    \"\"\"\n",
    "    r = requests[0]\n",
    "    t = r['qid']\n",
    "    clean_rs = [r,]\n",
    "    prev_t = t\n",
    "    for r in requests[1:]:\n",
    "        t = r['qid']\n",
    "        if t == prev_t:\n",
    "            continue\n",
    "        else:\n",
    "            clean_rs.append(r)\n",
    "            prev_t = t\n",
    "    return clean_rs\n",
    "\n",
    "def filter_blacklist_qid(requests):\n",
    "    \"\"\"\n",
    "    If the session contains an article in the blacklist,\n",
    "    drop the session. Currently, only the Main Page is\n",
    "    in the black list\n",
    "    \"\"\"\n",
    "\n",
    "    black_list = set(['Q5296',])\n",
    "    for r in requests:\n",
    "        if r['qid'] in black_list:\n",
    "            return False\n",
    "    return True\n",
    "   \n",
    "\n",
    "def sessionize(requests, dt = 3600):\n",
    "    \"\"\"\n",
    "    Break request stream whenever\n",
    "    there is a gap larger than dt [secs] in requests.\n",
    "    default is 3600s=1hour [from Halfaker et al. 2015]\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "    session = [requests[0]]\n",
    "    for r in requests[1:]:\n",
    "        d = r['ts'] -  session[-1]['ts']\n",
    "        if d > datetime.timedelta(seconds=dt):\n",
    "            sessions.append(session)\n",
    "            session = [r,]\n",
    "        else:\n",
    "            session.append(r)\n",
    "\n",
    "    sessions.append(session)\n",
    "    return sessions    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the actor-table\n",
    "- get pageviews\n",
    "- join wikidata-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wiki_db ## seleecting wikidata gets pageviews from all wikis\n",
    "## select a specific wiki_db for testing to reduce processing time, e.g. simplewiki\n",
    "# wiki_db = 'simplewiki'\n",
    "wiki_db = 'wikidata'\n",
    "\n",
    "# wikidata snapshot (for matching qids)\n",
    "wikidata_snapshot = \"2022-10-24\"\n",
    "\n",
    "## timewindow\n",
    "date_start = datetime.datetime(2022, 10, 1, 0)\n",
    "date_end = datetime.datetime(2022, 11, 1, 0)\n",
    "date_start_str = date_start.strftime('%Y-%m-%d-%H')\n",
    "date_end_str = date_end.strftime('%Y-%m-%d-%H')\n",
    "\n",
    "ts_start = calendar.timegm(date_start.timetuple())\n",
    "ts_end = calendar.timegm(date_end.timetuple())\n",
    "row_timestamp = F.unix_timestamp(F.concat(\n",
    "    F.col('year'), F.lit('-'), F.col('month'), F.lit('-'), F.col('day'), \n",
    "    F.lit(' '), F.col('hour'), F.lit(':00:00')))\n",
    "\n",
    "## filter pageviews from actor with more than 500 pageviews\n",
    "## the aim is to filter automated traffic that is not tagged as spider\n",
    "w_p = Window.partitionBy(F.col('actor_signature_per_project_family'), F.col('year'), F.col('month'), F.col('day'))\n",
    "n_p_max = 500 ## maximum number of pageviews/user/day\n",
    "n_p_min = 1 ## minimum number of pageviews/user/day\n",
    "\n",
    "# ## filter pageviews of articles with less than 50 different actors (clients)\n",
    "## the aim is to ensure privacy\n",
    "## actor signature is unique across projects (unique)\n",
    "w_c = Window.partitionBy(F.col('wiki_db'), F.col('page_id'))\n",
    "n_c_min = 50 ## minimum number of different clients per article\n",
    "\n",
    "## filtering sessions\n",
    "dt = 3600 ## cutoff for splitting sessions(interevent time between 2 pageivews)\n",
    "nlen_min = 2 ## min length of session\n",
    "nlen_max = 30 ## max length of session\n",
    "\n",
    "PATH_OUT = '/home/mgerlach/REPOS/reader-embedding-api/output/sessions/'\n",
    "filename_save = 'reading-sessions-actors_%s_%s_%s'%(wiki_db,date_start_str,date_end_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actor = (\n",
    "    spark.read.table('wmf.pageview_actor')\n",
    "    .where(row_timestamp >= ts_start)\n",
    "    .where(row_timestamp < ts_end)\n",
    "    .where(F.col('is_pageview')==True)\n",
    "    ## agent-type user to filter spiders\n",
    "    ## https://meta.wikimedia.org/wiki/Research:Page_view/Tags#Spider\n",
    "    .where(F.col('agent_type') == \"user\")\n",
    "    ## user: desktop/mobile/mobile app; isaac filters != mobile app\n",
    "    .where(F.col('access_method') != \"mobile app\")\n",
    "    ## only wikis\n",
    "    .where(F.col('normalized_host.project_family')=='wikipedia')\n",
    "    ## only namespace 0\n",
    "    .where( F.col('namespace_id') == 0 )\n",
    "    .withColumn('wiki_db', F.concat(F.col('normalized_host.project'),F.lit('wiki')) )\n",
    ")\n",
    "## filter only specific wiki (or all if wiki_db=='wikidata')\n",
    "if wiki_db == 'wikidata':\n",
    "    pass\n",
    "else:\n",
    "    df_actor = df_actor.where(F.col('wiki_db')==wiki_db)\n",
    "    \n",
    "## filter maximum and minimum pageviews per user\n",
    "## n_p is the number of pageviews per actor per day (across projects)\n",
    "df_actor = (\n",
    "    df_actor\n",
    "    .withColumn('n_p', F.sum(F.lit(1)).over(w_p) )\n",
    "    .where(F.col('n_p') >= n_p_min)\n",
    "    .where(F.col('n_p') <= n_p_max)    \n",
    ")\n",
    "\n",
    "## filter pages with minimum number of clients\n",
    "## n_c is the number of different actor-signatures per article (page_id+wiki_db)\n",
    "df_actor = (\n",
    "    df_actor\n",
    "#     .withColumn('n_c', F.size(F.collect_set('actor_signature').over(w_c)) ) ## this approach is very slow!!!\n",
    "    .withColumn('n_c', F.approx_count_distinct(F.col('actor_signature')).over(w_c) ) ##  this is faster but only approximate\n",
    "    ## we can specify a relative error bound (lower error==more time)\n",
    "    ##https://databricks.com/blog/2016/05/19/approximate-algorithms-in-apache-spark-hyperloglog-and-quantiles.html\n",
    "    .where(F.col('n_c') >= n_c_min)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join the wikidata-item to each pageview\n",
    "## we keep only pageviews for which we have a correpsionding wikidata-item id\n",
    "\n",
    "## table with mapping wikidata-ids to page-ids\n",
    "## partition wikidb and page-id ordered by snapshot\n",
    "w_wd = Window.partitionBy(F.col('wiki_db'),F.col('page_id')).orderBy(F.col('snapshot').desc())\n",
    "df_wd = (\n",
    "    spark.read.table('wmf.wikidata_item_page_link')\n",
    "    ## snapshot: this is a partition!\n",
    "    .where(F.col('snapshot') == wikidata_snapshot) ## resolve issues with non-mathcing wikidata-items\n",
    "    ## only wikis (enwiki, ... not: wikisource)\n",
    "    .where(F.col('wiki_db').endswith('wiki'))\n",
    ")\n",
    "## filter only specific wiki (or all if wiki_db=='wikidata')\n",
    "if wiki_db == 'wikidata':\n",
    "    pass\n",
    "else:\n",
    "    df_wd = df_wd.where(F.col('wiki_db')==wiki_db)\n",
    "## get the most recent wikidata-item for each pid+wikidb\n",
    "df_wd = (\n",
    "    df_wd\n",
    "    .withColumn('item_id_latest',F.first(F.col('item_id')).over(w_wd))\n",
    "    .select(\n",
    "        'wiki_db',\n",
    "        'page_id',\n",
    "        F.col('item_id_latest').alias('item_id')\n",
    "    )\n",
    "    .drop_duplicates()\n",
    ")\n",
    "df_actor_wd = (\n",
    "    df_actor\n",
    "    .join(\n",
    "        df_wd,\n",
    "        on = ['page_id','wiki_db'],\n",
    "        how='inner'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregate all pageviews with same actor-signature across wikis to get sessions\n",
    "df_actor_wd_agg = (\n",
    "    df_actor_wd\n",
    "    .groupby('actor_signature_per_project_family')\n",
    "    .agg(\n",
    "         F.first(F.col('access_method')).alias('access_method'), ## this could change along a session\n",
    "         F.first(F.col('geocoded_data')).alias('geocoded_data'),\n",
    "#              F.first(F.col('n_p_by_user')).alias('session_length'),\n",
    "         F.array_sort(\n",
    "             F.collect_list(\n",
    "                 F.struct(\n",
    "                     F.col('ts'),\n",
    "                     F.col('page_id'),\n",
    "                     F.col('pageview_info.page_title').alias('page_title'),\n",
    "                     F.col('wiki_db'),\n",
    "                     F.col('item_id').alias('qid'),\n",
    "                 )\n",
    "             )\n",
    "         ).alias('session')\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply filter to the sessions\n",
    "try:\n",
    "    os.mkdir(PATH_OUT) \n",
    "except FileExistsError:\n",
    "    pass\n",
    "PATH_TMP = os.path.join(PATH_OUT,'tmp')\n",
    "try:\n",
    "    os.mkdir(PATH_TMP) \n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "## hdfs-storing, some temporary files which will be deleted later\n",
    "base_dir_hdfs = '/user/mgerlach/sessions'\n",
    "output_hdfs_dir = os.path.join(base_dir_hdfs,filename_save)\n",
    "os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "## local storing\n",
    "base_dir_local =  PATH_OUT\n",
    "output_local_dir_tmp = os.path.join(base_dir_local,'tmp',filename_save)\n",
    "output_local_file = os.path.join(base_dir_local,filename_save)\n",
    "\n",
    "## load data\n",
    "# requests = spark.read.load(filename).rdd.map(lambda x: x['session'])\n",
    "requests = df_actor_wd_agg.rdd.map(lambda x: x['session'])\n",
    "## keep only pageviews from a language\n",
    "requests = requests.map(lambda rs: [r for r in rs if r['qid'] != None])\n",
    "to_str = lambda x: ' '.join([str(e['qid']) for e in x])\n",
    "\n",
    "(requests\n",
    " .map(parse_requests)\n",
    " .filter(filter_blacklist_qid) ## remove main_page\n",
    " .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    " .map(filter_consecutive_articles) ## remove consecutive calls to same article\n",
    " .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    " .flatMap(lambda x: sessionize(x, dt = dt)) ## break sessions if interevent time is too large\n",
    " .filter(lambda x: len(x)>=nlen_min) ## only sessions with at least length nlen_min\n",
    " .filter(lambda x: len(x)<=nlen_max) ## only sessions with at most length nlen_max\n",
    " .map(to_str) ## conctenate session as single string\n",
    " ## write to hdfs\n",
    " .saveAsTextFile(output_hdfs_dir,compressionCodecClass = \"org.apache.hadoop.io.compress.GzipCodec\")\n",
    ")\n",
    "\n",
    "## copy to local (set of tmp-dirs)\n",
    "os.system('hadoop fs -copyToLocal %s %s'%(output_hdfs_dir,output_local_dir_tmp))\n",
    "## concatenate and unzip into single file\n",
    "os.system('cat %s/* | gunzip > %s'%(output_local_dir_tmp,output_local_file))\n",
    "## remove set of tmp-dirs\n",
    "os.system('rm -rf %s'%output_local_dir_tmp)\n",
    "## remove hadoop data\n",
    "os.system('hadoop fs -rm -r %s'%output_hdfs_dir)\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_conda-base",
   "language": "python",
   "name": "venv_conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
